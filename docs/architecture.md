# EY Data Platform Architecture

## Executive Summary

Production-ready data platform built on Google Cloud Platform that ingests e-commerce data from REST APIs, transforms it using modern data stack principles, and delivers analytics-ready star schema for business intelligence.

## Architecture Overview

![Architecture Diagram](architecture.mmd)

*Mermaid diagram showing complete data flow from APIs to analytics tables*

## Technical Architecture

### ğŸ—ï¸ **Multi-Layer Design**

```
APIs â†’ Ingestion â†’ Raw Storage â†’ Transformations â†’ Analytics
```

1. **External APIs**: FakeStore API endpoints (Products, Users, Carts)
2. **Ingestion Layer**: Python-based batch and streaming processors
3. **Raw Storage**: Immutable JSONL files + BigQuery raw tables
4. **Transformation Layer**: dbt staging and marts models
5. **Analytics Layer**: Star schema optimized for BI tools

### ğŸ”„ **Data Ingestion Patterns**

#### Batch Processing (Products & Users)
```python
Schedule: Daily (idempotent)
Pattern: API â†’ GCS â†’ BigQuery
Files: Date-partitioned JSONL
Records: 30 total (20 products + 10 users)
```

#### Streaming Processing (Cart Events)
```python
Frequency: 60-second polling
Pattern: API â†’ Pub/Sub â†’ BigQuery
Deduplication: processed_cart_ids tracking
Metadata: event_id, extracted_at, published_at
```

### ğŸ“Š **Data Models**

#### Raw Layer (`bq_raw` dataset)
- **Products**: Product catalog data
- **Users**: Customer information  
- **carts_events**: Real-time shopping cart activities

#### Staging Layer (dbt views)
- **stg_products**: Cleaned product data
- **stg_users**: Standardized user profiles
- **stg_cart_events**: Processed cart events

#### Analytics Layer (`bq_analytics` dataset)
- **dim_products**: Product dimension (20 rows)
- **dim_users**: User dimension (10 rows)
- **fact_orders**: Order facts (date-partitioned)

## Technology Stack

### **Core Technologies**
- **Python 3.13**: Primary programming language
- **dbt 1.11**: Data transformation framework
- **Google Cloud Platform**: Cloud infrastructure
- **Pydantic**: Data validation and modeling

### **GCP Services**
- **BigQuery**: Data warehouse and analytics engine
- **Cloud Storage**: Raw data lake storage
- **Pub/Sub**: Real-time message streaming
- **IAM**: Authentication and authorization

### **Development Tools**
- **Git**: Version control and collaboration
- **Virtual Environment**: Dependency isolation
- **Environment Variables**: Configuration management

## Data Flow Patterns

### ğŸ”„ **Batch Data Flow**
```
FakeStore API â†’ Batch Pipeline â†’ Cloud Storage â†’ BigQuery â†’ dbt â†’ Analytics Tables
```

### âš¡ **Streaming Data Flow**
```
Carts API â†’ Cart Poller â†’ Pub/Sub â†’ Stream Consumer â†’ BigQuery â†’ dbt â†’ Fact Tables
```

## Infrastructure Components

### **BigQuery Datasets**
- `bq_raw`: Raw ingested data (source of truth)
- `bq_analytics`: Transformed analytics tables (BI ready)

### **Cloud Storage Buckets**
- `ey-data-platform-raw-data`: JSONL file storage

### **Pub/Sub Resources**
- Topic: `carts-events`
- Subscription: `carts-events-sub`

### **Service Account Permissions**
- BigQuery Admin (data loading and querying)
- Storage Admin (JSONL file operations)
- Pub/Sub Admin (message publishing and consuming)

## Design Principles

### **ğŸ”’ Data Reliability**
- **Idempotent Processing**: Batch jobs safe to re-run
- **Event Deduplication**: Cart events processed only once
- **Immutable Storage**: Raw data never modified
- **Schema Validation**: Pydantic models ensure data quality

### **ğŸ“ˆ Scalability**
- **Partitioned Tables**: Date-based partitioning for performance
- **Streaming Inserts**: Real-time data availability
- **Modular Architecture**: Independent scaling of components
- **Cloud-Native**: Leverages managed GCP services

### **ğŸ› ï¸ Maintainability**
- **Infrastructure as Code**: Reproducible deployments
- **Version Control**: Complete codebase in Git
- **Documentation**: Comprehensive technical docs
- **Testing**: End-to-end validation scripts

## Performance Characteristics

### **Throughput**
- Batch: 30 records/day (API limitations)
- Streaming: 7 carts/minute polling rate
- dbt: 6 models in 11.37 seconds

### **Latency**
- Batch: 24-hour SLA (daily refresh)
- Streaming: <2 minutes (polling + processing)
- Analytics: Real-time (updated on dbt run)

### **Storage**
- Raw JSONL: ~10KB per day
- BigQuery: Compressed columnar storage
- Analytics: Star schema for query optimization

## Security Implementation

### **Authentication**
- Service Account with minimal required permissions
- Environment-based credential management
- No hardcoded secrets in codebase

### **Data Access**
- BigQuery dataset-level permissions
- GCS bucket-level access controls
- Pub/Sub topic-level authorization

### **Network Security**
- HTTPS for all API communications
- GCP internal networking for service communication
- No public endpoints exposed

## Monitoring & Observability

### **Current Implementation**
- Python logging for pipeline execution
- BigQuery audit logs for data access
- dbt run results and documentation

### **Production Enhancements**
- GCP Monitoring for infrastructure metrics
- Data quality monitoring with Great Expectations
- Alerting for pipeline failures
- Cost monitoring and optimization

## Deployment Strategy

### **Current Approach**
- Manual service account setup
- Local development environment
- Git-based code management

### **Production Readiness**
- Infrastructure as Code for GCP resource provisioning
- CI/CD pipelines for automated deployment
- Environment promotion (dev â†’ staging â†’ prod)
- Blue/green deployment for zero-downtime updates